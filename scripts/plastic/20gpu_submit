#!/bin/bash
#SBATCH --job-name=plastic
#SBATCH --output=md_%j.log     
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=20
#SBATCH --partition=gpuGTX
#SBATCH --exclude=compute-21-1,compute-18-1,compute-17-1,compute-20-1
#SBATCH --gpus-per-node=1

export TMPDIR=/state/partition1
source /home/spack-user/spack/share/spack/setup-env.sh
spack load gromacs/z7lm

echo "SLURM_JOBID=$SLURM_JOBID"
echo "SLURM_JOB_NODELIST=$SLURM_JOB_NODELIST"
echo "SLURM_NNODES=$SLURM_NNODES "
echo "working directory= $SLURM_SUBMIT_DIR "
echo "tmp directory=$TMPDIR"
echo "NPROCS=$SLURM_NPROCS"
echo "Number of CPUs per task=$SLURM_CPUS_PER_TASK"
echo "Count of processors avaiable to the job on this node=$SLURM_JOB_CPUS_PER_NODE"
echo "Number of CPUs requested per allocated GPU=$SLURM_CPUS_PER_GPU"
echo "Number of GPUs requested=$SLURM_GPUS"
echo "Number of tasks=$SLURM_NTASKS"

# no coredumps
ulimit -S -c 0
ulimit -s unlimited
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

mkdir -p $TMPDIR/$USER/$SLURM_JOBID
cp -rf $SLURM_SUBMIT_DIR/* $TMPDIR/$USER/$SLURM_JOBID
cd $TMPDIR/$USER/$SLURM_JOBID

# run the simulation
./simulate.sh

cp -rf $TMPDIR/$USER/$SLURM_JOBID/* $SLURM_SUBMIT_DIR/

rm -rf $TMPDIR/$USER/$SLURM_JOBID
exit

